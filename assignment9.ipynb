{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"Technology is evolving rapidly. I enjoy learning about AI, robotics, and space exploration! Books about tech are fascinating. Food tech innovations are also exciting. Sports technology is changing the game.\"\n",
    "\n",
    "text_lower = text.lower()\n",
    "text_clean = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "\n",
    "words = word_tokenize(text_clean)\n",
    "sentences = sent_tokenize(text_clean)\n",
    "\n",
    "filtered_words = [w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "word_freq = Counter(filtered_words)\n",
    "word_freq\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "porter_stemmed = [porter.stem(w) for w in filtered_words]\n",
    "lancaster_stemmed = [lancaster.stem(w) for w in filtered_words]\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "\n",
    "porter_stemmed\n",
    "lancaster_stemmed\n",
    "lemmatized\n",
    "\n",
    "words_more_than_5 = [w for w in filtered_words if len(w) > 5]\n",
    "numbers_in_text = re.findall(r'\\d+', text_clean)\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "\n",
    "alphabets_only = re.findall(r'[A-Za-z]+', text_clean)\n",
    "words_starting_vowel = [w for w in alphabets_only if w[0] in 'aeiou']\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    text = re.sub(r'(\\w)-(\\w)', r'\\1\\2', text)\n",
    "    tokens = re.findall(r'\\d+\\.\\d+|\\d+|\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "custom_tokens = custom_tokenizer(text)\n",
    "\n",
    "def replace_patterns(text):\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub(r'(\\+?\\d{1,3}[-\\s]?)?\\d{10}|\\d{3}[-\\s]\\d{3}[-\\s]\\d{4}', '<PHONE>', text)\n",
    "    return text\n",
    "\n",
    "text_replaced = replace_patterns(text)\n",
    "text_replaced\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
